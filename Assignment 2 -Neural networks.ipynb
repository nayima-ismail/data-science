{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72bbc18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow and Keras are two packages for creating neural network models.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import NN layers and other componenets.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "import matplotlib.pyplot as plt # for plotting data and creating different charts.\n",
    "import numpy as np # for math and arrays\n",
    "import pandas as pd # data from for the data.\n",
    "import seaborn as sns # for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc892bf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>58</td>\n",
       "      <td>0.582016</td>\n",
       "      <td>0.351544</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>66</td>\n",
       "      <td>0.073498</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>97</td>\n",
       "      <td>0.832630</td>\n",
       "      <td>0.110514</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>67</td>\n",
       "      <td>0.296772</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>51</td>\n",
       "      <td>0.305414</td>\n",
       "      <td>0.335498</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>83</td>\n",
       "      <td>0.092107</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>16</td>\n",
       "      <td>0.175202</td>\n",
       "      <td>0.431177</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>31</td>\n",
       "      <td>0.246629</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>9</td>\n",
       "      <td>0.041532</td>\n",
       "      <td>0.254211</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>62</td>\n",
       "      <td>0.301854</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>6</td>\n",
       "      <td>0.796317</td>\n",
       "      <td>0.620558</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>32</td>\n",
       "      <td>0.503962</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>42</td>\n",
       "      <td>0.649129</td>\n",
       "      <td>0.033726</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>81</td>\n",
       "      <td>0.575259</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>28</td>\n",
       "      <td>0.423688</td>\n",
       "      <td>0.518020</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>55</td>\n",
       "      <td>0.473132</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>74</td>\n",
       "      <td>0.461396</td>\n",
       "      <td>0.408169</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>54</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>50</td>\n",
       "      <td>0.868384</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>125</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>2</td>\n",
       "      <td>0.288525</td>\n",
       "      <td>0.359993</td>\n",
       "      <td>...</td>\n",
       "      <td>54</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>68</td>\n",
       "      <td>0.788383</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3    4     5      6   7         8         9   ...  21  22  23  \\\n",
       "0   6  148  72  35    0  33.6  0.627  58  0.582016  0.351544  ...  50  51  35   \n",
       "1   1   85  66  29    0  26.6  0.351  97  0.832630  0.110514  ...  31  32  29   \n",
       "2   8  183  64   0    0  23.3  0.672  51  0.305414  0.335498  ...  32  33   0   \n",
       "3   1   89  66  23   94  28.1  0.167  16  0.175202  0.431177  ...  21  22  23   \n",
       "4   0  137  40  35  168  43.1  2.288   9  0.041532  0.254211  ...  33  34  35   \n",
       "5   5  116  74   0    0  25.6  0.201   6  0.796317  0.620558  ...  30  31   0   \n",
       "6   3   78  50  32   88  31.0  0.248  42  0.649129  0.033726  ...  26  27  32   \n",
       "7  10  115   0   0    0  35.3  0.134  28  0.423688  0.518020  ...  29  30   0   \n",
       "8   2  197  70  45  543  30.5  0.158  74  0.461396  0.408169  ...  53  54  45   \n",
       "9   8  125  96   0    0   0.0  0.232   2  0.288525  0.359993  ...  54  55   0   \n",
       "\n",
       "    24    25     26  27        28  29  30  \n",
       "0    0  33.6  0.627  66  0.073498   1   1  \n",
       "1    0  26.6  0.351  67  0.296772   1   2  \n",
       "2    0  23.3  0.672  83  0.092107   6   3  \n",
       "3   94  28.1  0.167  31  0.246629   8   4  \n",
       "4  168  43.1  2.288  62  0.301854   3   5  \n",
       "5    0  25.6  0.201  32  0.503962   9   1  \n",
       "6   88  31.0  0.248  81  0.575259   4   2  \n",
       "7    0  35.3  0.134  55  0.473132   7   3  \n",
       "8  543  30.5  0.158  50  0.868384   6   4  \n",
       "9    0   0.0  0.232  68  0.788383   6   5  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"pima-indians.csv\", header=None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7f81546",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       768 non-null    int64  \n",
      " 1   1       768 non-null    int64  \n",
      " 2   2       768 non-null    int64  \n",
      " 3   3       768 non-null    int64  \n",
      " 4   4       768 non-null    int64  \n",
      " 5   5       768 non-null    float64\n",
      " 6   6       768 non-null    float64\n",
      " 7   7       768 non-null    int64  \n",
      " 8   8       768 non-null    float64\n",
      " 9   9       768 non-null    float64\n",
      " 10  10      768 non-null    float64\n",
      " 11  11      768 non-null    float64\n",
      " 12  12      768 non-null    float64\n",
      " 13  13      768 non-null    float64\n",
      " 14  14      768 non-null    float64\n",
      " 15  15      768 non-null    float64\n",
      " 16  16      768 non-null    int64  \n",
      " 17  17      768 non-null    float64\n",
      " 18  18      768 non-null    int64  \n",
      " 19  19      768 non-null    int64  \n",
      " 20  20      768 non-null    int64  \n",
      " 21  21      768 non-null    int64  \n",
      " 22  22      768 non-null    int64  \n",
      " 23  23      768 non-null    int64  \n",
      " 24  24      768 non-null    int64  \n",
      " 25  25      768 non-null    float64\n",
      " 26  26      768 non-null    float64\n",
      " 27  27      768 non-null    int64  \n",
      " 28  28      768 non-null    float64\n",
      " 29  29      768 non-null    int64  \n",
      " 30  30      768 non-null    int64  \n",
      "dtypes: float64(14), int64(17)\n",
      "memory usage: 186.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb014929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>48.911458</td>\n",
       "      <td>0.502363</td>\n",
       "      <td>0.509221</td>\n",
       "      <td>...</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>34.240885</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>49.942708</td>\n",
       "      <td>0.499174</td>\n",
       "      <td>5.691406</td>\n",
       "      <td>2.555990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>29.356772</td>\n",
       "      <td>0.284582</td>\n",
       "      <td>0.285739</td>\n",
       "      <td>...</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>28.204863</td>\n",
       "      <td>0.297853</td>\n",
       "      <td>2.869128</td>\n",
       "      <td>1.385148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>...</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003838</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.255150</td>\n",
       "      <td>0.265821</td>\n",
       "      <td>...</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>26.750000</td>\n",
       "      <td>0.239481</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0.506342</td>\n",
       "      <td>0.495043</td>\n",
       "      <td>...</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>0.500552</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>0.744777</td>\n",
       "      <td>0.745594</td>\n",
       "      <td>...</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>0.753453</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.999659</td>\n",
       "      <td>0.999098</td>\n",
       "      <td>...</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
       "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
       "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
       "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
       "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
       "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
       "\n",
       "               6           7           8           9   ...          21  \\\n",
       "count  768.000000  768.000000  768.000000  768.000000  ...  768.000000   \n",
       "mean     0.471876   48.911458    0.502363    0.509221  ...   33.240885   \n",
       "std      0.331329   29.356772    0.284582    0.285739  ...   11.760232   \n",
       "min      0.078000    1.000000    0.001607    0.002182  ...   21.000000   \n",
       "25%      0.243750   23.000000    0.255150    0.265821  ...   24.000000   \n",
       "50%      0.372500   47.000000    0.506342    0.495043  ...   29.000000   \n",
       "75%      0.626250   75.000000    0.744777    0.745594  ...   41.000000   \n",
       "max      2.420000  100.000000    0.999659    0.999098  ...   81.000000   \n",
       "\n",
       "               22          23          24          25          26          27  \\\n",
       "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
       "mean    34.240885   20.536458   79.799479   31.992578    0.471876   49.942708   \n",
       "std     11.760232   15.952218  115.244002    7.884160    0.331329   28.204863   \n",
       "min     22.000000    0.000000    0.000000    0.000000    0.078000    1.000000   \n",
       "25%     25.000000    0.000000    0.000000   27.300000    0.243750   26.750000   \n",
       "50%     30.000000   23.000000   30.500000   32.000000    0.372500   51.000000   \n",
       "75%     42.000000   32.000000  127.250000   36.600000    0.626250   73.000000   \n",
       "max     82.000000   99.000000  846.000000   67.100000    2.420000  100.000000   \n",
       "\n",
       "               28          29          30  \n",
       "count  768.000000  768.000000  768.000000  \n",
       "mean     0.499174    5.691406    2.555990  \n",
       "std      0.297853    2.869128    1.385148  \n",
       "min      0.003838    1.000000    0.000000  \n",
       "25%      0.239481    3.000000    2.000000  \n",
       "50%      0.500552    6.000000    2.000000  \n",
       "75%      0.753453    8.000000    4.000000  \n",
       "max      0.999775   10.000000    5.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c168ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = df.iloc[:, 0:29]\n",
    "Y_data = df.iloc[:, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01568ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((614, 29), (154, 29))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X,test_X,train_Y,test_Y = train_test_split(X_data, Y_data, test_size=0.2, random_state=13)\n",
    "\n",
    "train_X.shape,valid_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "379af1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38, 29), (39, 29))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the test_dataset dataframe to 50% test and 50% validation. [this will divide the dataset into 60% train, 20% validate, and 20% test]\n",
    "test_X,valid_X,test_Y,valid_Y =  train_test_split(test_X,test_Y, test_size=0.5)\n",
    "test_X.shape,valid_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a15bffbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: [0 1 2 3 4 5]\n",
      "After conversion to one-hot: [[0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Change the labels from categorical to one-hot encoding\n",
    "train_Y_one_hot = to_categorical(train_Y, num_classes = 6)\n",
    "test_Y_one_hot = to_categorical(test_Y, num_classes = 6)\n",
    "valid_Y_one_hot = to_categorical(valid_Y, num_classes = 6)\n",
    "\n",
    "# Display the change for category label using one-hot encoding\n",
    "print('Original label:', np.unique(train_Y))\n",
    "print('After conversion to one-hot:', train_Y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a381cde2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       3.845277\n",
       "1     120.325733\n",
       "2      69.605863\n",
       "3      20.767101\n",
       "4      77.408795\n",
       "5      31.964984\n",
       "6       0.464200\n",
       "7      50.066775\n",
       "8       0.516622\n",
       "9       0.510510\n",
       "10     34.184183\n",
       "11     13.585230\n",
       "12     20.498976\n",
       "13     79.464027\n",
       "14     31.893646\n",
       "15      0.462671\n",
       "16     49.495114\n",
       "17      0.495525\n",
       "18      5.703583\n",
       "19     33.143322\n",
       "20      5.503257\n",
       "21     33.143322\n",
       "22     34.143322\n",
       "23     20.767101\n",
       "24     77.408795\n",
       "25     31.964984\n",
       "26      0.464200\n",
       "27     49.495114\n",
       "28      0.495525\n",
       "Name: mean, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stats = train_X.describe()\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f77088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to normalize the data set.\n",
    "def norm(x):\n",
    "    return (x - train_stats['mean']) / train_stats['std']\n",
    "\n",
    "normed_train_data = norm(train_X)\n",
    "normed_test_data = norm(test_X)\n",
    "normed_valid_dataset = norm(valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cf74017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of outputs :  6\n",
      "Output classes :  [0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# Find the unique numbers from the train labels\n",
    "classes = np.unique(train_Y)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4fb0932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of this model: \n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 30)                900       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 18)                558       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 9)                 171       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 6)                 60        \n",
      "=================================================================\n",
      "Total params: 1,689\n",
      "Trainable params: 1,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We decalred a function for creating a model.\n",
    "def build_model1_four_hidden_layers():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_shape = (normed_train_data.shape[1],)))    # Input layer    \n",
    "    model.add(Dense(18, activation='softmax'))  \n",
    "    model.add(Dense(9, activation='softmax'))   \n",
    "    model.add(Dense(nClasses, activation='softmax'))\n",
    "    \n",
    "\n",
    "    learning_rate = 0.0001\n",
    "    optimizer = optimizers.Adam(learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "batch_size = 16 # 6 iteration\n",
    "\n",
    "model = build_model1_four_hidden_layers()\n",
    "print('Here is a summary of this model: ')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bbddfa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 50\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53fbbfb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (614, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-a7eefa0be7ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormed_train_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormed_test_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m     return func.fit(\n\u001b[0m\u001b[0;32m    796\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    617\u001b[0m                                                      steps_per_epoch, x)\n\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m     x, y, sample_weights = model._standardize_user_data(\n\u001b[0m\u001b[0;32m    620\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2345\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2347\u001b[1;33m     return self._standardize_tensors(\n\u001b[0m\u001b[0;32m   2348\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2349\u001b[0m         \u001b[0mrun_eagerly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2456\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2457\u001b[0m           \u001b[1;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2458\u001b[1;33m           training_utils_v1.check_loss_and_target_compatibility(\n\u001b[0m\u001b[0;32m   2459\u001b[0m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0;32m   2460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils_v1.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    804\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_categorical_crossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m         raise ValueError('You are passing a target array of shape ' +\n\u001b[0m\u001b[0;32m    807\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m                          \u001b[1;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are passing a target array of shape (614, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": [
    "model_train = model.fit(normed_train_data, train_Y_one_hot, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(normed_test_data, valid_Y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
