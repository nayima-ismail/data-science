1. Topic modelling based on reviews on Amazon

Problem statement: Online shopping is all over the internet. All our needs are just a click away. The biggest online shopping website Amazon known for its variety of products and sells more than lacs of product each day.
Amazon used to collect the feedback/tweets/texts from each customer though various medium like facebook, email etc. Automatic segmentation of this  text would benefit on their management process.  
The objective of this project is to conduct LDA topic modelling find various topics that are present in your corpus and each document in the corpus will be made up of at least one topic
Dataset: reviews.csv
This is a subset data set of Amazon Review Data 

2. Story Telling with Netflix data
Problem statement: With a company like Netflix that is brimming with data, it’s always a wise decision to put that pile of data to good use . The aim of this project to conduct 
Exploratory Data analysis
Construct the wordcloud for the country, cast in the shows, Directors and movie categories

Dataset : netflix_titles.csv
https://www.kaggle.com/shivamb/netflix-shows 
This dataset collected from tv shows and movies available on Netflix search engine as of 2019. 

3. Personality classification
Background : The Myers–Briggs Type Indicator (MBTI) is a kind of psychological classification about humans experience using four principal psychological functions, sensation, intuition, feeling, and thinking, constructed by Katharine Cook Briggs and her daughter Isabel Briggs Myers. 
The Myers Briggs Type Indicator (or MBTI for short) is a personality type system that divides everyone into 16 distinct personality types across 4 axis:
Introversion (I) – Extroversion (E)
Intuition (N) – Sensing (S)
Thinking (T) – Feeling (F)
Judging (J) – Perceiving (P)
The purpose of the Myers-Briggs Type Indicator® (MBTI®) personality inventory is to make the theory of psychological types described by C. G. Jung understandable and useful in people's lives

Problem statement: The objective of this model to conduct exploratory data analysis and basic  text analytics Based on Word Clouds from the given dataset.

Dataset: mbti1_csv
https://www.kaggle.com/datasnaek/mbti-type

4. Regularize the coefficient of IPL auction analytics
o	This helps to avoid over fitting of the model
Data Description:
The IPL is professional cricket t20 league championship started in India in 2008. It was initiated by BCCI with 8 franchises comprising players across the world. The first IPL auction was held in 2008  for ownership of the teams for 10 years, with base price of USD 50 Billion. The franchises acquire players through an auction that is conducted every year. However , there are several rules imposed by the  IPL. The performance of the players could be measured many ways. Although the IPL follows twenty20 format of the game, it is possible that the performance of the players in the other format of the game such as test and ODI matches can influence the player pricing   The data set consist of performance 0f 130 players measured through various performance measure metrics such as batting , bowling etc.  
Data set name: IPL IMB381IPL2013.CSV
Objective: Build model  and  egularize the coefficient to understand what features of players are influencing their SOLD PRICE  or predict the player’s auction price in future. 
Activities to be performed
•	Loading data set
•	Standardization of columns
•	Encoding categorical features
•	Building model on the train data set
•	Plotting the coefficient values and RMSE
•	Handling Multi collinearity using VIFs
•	Building the model after removing Multi collinearity
•	Residual analysis using P-P plot
•	Apply regularization: Ridge, LASSO, elasticnet
•	Detecting influencers
•	Transforming response variable 

5. We will be using the customer churn data from the telecom industry that we used in week 1 for this week's exercises. The data file is called `Orange_Telecom_Churn_Data.csv`.
Activities to be included:
•	Import the customer churn data, which is found in the file Orange_Telecom_Churn_Data.csv. 
•	Remove any columns that are likely not to be used for prediction.
•	Encode data types as appropriate
•	Examine distribution of the predicted variable (churned).
•	Split the data into train and test sets. Decide if a stratified split should be used or not based on the distribution.
•	Examine the distribution of the predictor variable in the train and test data.
•	Fit random forest models with a range of tree numbers and evaluate the out-of-bag error for each of these models.
•	Plot the resulting oob errors as a function of the number of trees.
•	Repeat question 3 using extra randomized trees (ExtraTreesClassifier). Note that the bootstrap parameter will have to be set to True for this model.
•	Compare the out-of-bag errors for the two different types of models.
•	Select one of the models that performs well and calculate error metrics and a confusion matrix on the test data set. 
•	Given the distribution of the predicted class, which metric is most important? Which could be deceiving?
•	Print or visualize the confusion matrix.
•	Plot the ROC-AUC and precision-recall curves.
•	Plot the feature importances.
•	Create different objects using multiple smoting methods
Ex: os_sm = os_smote.SMOTE(random_state=45, ratio = 0.6)
•	Fit random forest models with a range of tree numbers and evaluate the out-of-bag error for each of these models.
•	Plot the resulting oob errors as a function of the number of trees

